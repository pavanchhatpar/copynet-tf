{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict()\n",
    "\n",
    "cfg.GLOVE = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "cfg.DATA_DIR = '../../data/summarization/data'\n",
    "cfg.MODEL_DIR = '../../data/summarization/model'\n",
    "\n",
    "# from https://arxiv.org/abs/1704.04368\n",
    "cfg.ART_LEN = 781\n",
    "cfg.SUM_LEN = 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import wget\n",
    "import os\n",
    "\n",
    "os.chdir(\"/tf/src/examples/summarization\")\n",
    "\n",
    "if not os.path.exists(cfg.DATA_DIR):\n",
    "  os.makedirs(cfg.DATA_DIR)\n",
    "\n",
    "glovefile = os.path.join(cfg.DATA_DIR, \"glove\", \"glove.840B.300d.txt\")\n",
    "\n",
    "while not os.path.exists(glovefile):\n",
    "  glove_zip = os.path.join(cfg.DATA_DIR, \"glove.840B.300d.zip\")\n",
    "  glove_unzip = os.path.join(cfg.DATA_DIR, \"glove\")\n",
    "  wget.download(cfg.GLOVE, glove_zip)\n",
    "  with zipfile.ZipFile(glove_zip) as f:\n",
    "    f.extractall(glove_unzip)\n",
    "  os.remove(glove_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'test': <PrefetchDataset shapes: {article: (), highlights: ()}, types: {article: tf.string, highlights: tf.string}>,\n 'train': <PrefetchDataset shapes: {article: (), highlights: ()}, types: {article: tf.string, highlights: tf.string}>,\n 'validation': <PrefetchDataset shapes: {article: (), highlights: ()}, types: {article: tf.string, highlights: tf.string}>}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "data_dir = os.path.join(cfg.DATA_DIR, \"tf_data\")\n",
    "data = tfds.load('cnn_dailymail/plain_text', data_dir=data_dir)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copynet_tf import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class GloVeReader:\n",
    "    UNK = 'UNKNOWN'\n",
    "    PAD = 'PAD'\n",
    "    START = '<S>'\n",
    "    END = 'EOS'\n",
    "\n",
    "    def read(self,\n",
    "             filename: str) -> Dict[str, np.ndarray]:\n",
    "        data = {}\n",
    "        with open(filename, 'r') as fin:\n",
    "            for line in tqdm(fin, desc='Loading vectors'):\n",
    "                tokens = line.split(' ')\n",
    "                data[tokens[0].strip()] = np.array(\n",
    "                    tokens[1:], dtype=np.float32)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(vocab, data, fit=False, pretrained_vectors=None):\n",
    "  articles = []\n",
    "  highlights = []\n",
    "  print(\"Caching data...\")\n",
    "  for tup in data:\n",
    "    articles.append(tup['article'].numpy().decode())\n",
    "    highlights.append(tup['highlights'].numpy().decode())\n",
    "  print(\"Tokenizing data...\")\n",
    "  articles = list(nlp.pipe(articles, disable=[\"tagger\", \"parser\", \"ner\"], batch_size=1000))\n",
    "  highlights = list(nlp.pipe(highlights, disable=[\"tagger\", \"parser\", \"ner\"], batch_size=1000))\n",
    "  if fit:\n",
    "    print(\"Fitting vocab over tokens...\")\n",
    "    if pretrained_vectors is None:\n",
    "      raise ValueError(\"Give pretrained vectors while fitting\")\n",
    "    vocab.fit(articles, highlights, pretrained_vectors, 0, 5)\n",
    "  processed = zip(\n",
    "      vocab.transform(articles, \"source\"),\n",
    "      vocab.transform(articles, \"target\", vocab._source_seq_len),\n",
    "      vocab.transform(highlights, \"target\"),\n",
    "      vocab.transform(highlights, \"source\", vocab._target_seq_len),\n",
    "  )\n",
    "\n",
    "  def gen():\n",
    "    for X, Xt, y, yt in processed:\n",
    "      yield (X, Xt, y, yt)\n",
    "  \n",
    "  return tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      (tf.int32, tf.int32, tf.int32, tf.int32),\n",
    "      (tf.TensorShape([vocab._source_seq_len]),\n",
    "       tf.TensorShape([vocab._source_seq_len]),\n",
    "       tf.TensorShape([vocab._target_seq_len]),\n",
    "       tf.TensorShape([vocab._target_seq_len])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Loading vectors: 2196017it [03:15, 11210.26it/s]\n"
    }
   ],
   "source": [
    "reader = GloVeReader()\n",
    "pretrained_vectors = reader.read(os.path.join(cfg.DATA_DIR, \"glove\", \"glove.840B.300d.txt\"))\n",
    "vocab = Vocab(\n",
    "    reader.START, reader.END, reader.PAD, reader.UNK,\n",
    "    cfg.ART_LEN, cfg.SUM_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Caching data...\nTokenizing data...\nFitting vocab over tokens...\nsource max ('.', 11114933) min ('195-a', 1)\ntarget max ('.', 1028967) min ('Mimicking', 1)\n"
    }
   ],
   "source": [
    "train = process(vocab, data['train'], fit=True, pretrained_vectors=pretrained_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Caching data...\nTokenizing data...\n"
    }
   ],
   "source": [
    "val = process(vocab, data['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Caching data...\nTokenizing data...\n"
    }
   ],
   "source": [
    "test = process(vocab, data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "\n",
    "    # BytesList won't unpack a string from an EagerTensor.\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def pyserialize(X, Xt, y, yt):\n",
    "    X = tf.io.serialize_tensor(X)\n",
    "    y = tf.io.serialize_tensor(y)\n",
    "    Xt = tf.io.serialize_tensor(Xt)\n",
    "    yt = tf.io.serialize_tensor(yt)\n",
    "    feature = {\n",
    "        \"X\": bytes_feature(X),\n",
    "        \"y\": bytes_feature(y),\n",
    "        \"Xt\": bytes_feature(Xt),\n",
    "        \"yt\": bytes_feature(yt),\n",
    "    }\n",
    "    example_proto = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def serialize(X, Xt, y, yt):\n",
    "    serialized = tf.py_function(\n",
    "        pyserialize,\n",
    "        [X, Xt, y, yt],\n",
    "        tf.string\n",
    "    )\n",
    "    return tf.reshape(serialized, ())\n",
    "\n",
    "def save(vocab, train, test, val):\n",
    "    base_loc = os.path.join(cfg.DATA_DIR, \"prepared\")\n",
    "    if not os.path.exists(base_loc):\n",
    "        os.makedirs(base_loc)\n",
    "    print(\"******** Saving Vocabulary ********\")\n",
    "    vocab.save(os.path.join(base_loc, \"vocab\"))\n",
    "\n",
    "    print(\"******** Saving Validation set ********\")\n",
    "    fname = os.path.join(base_loc, \"val.tfrecord\")\n",
    "    writer = tf.data.experimental.TFRecordWriter(fname, \"ZLIB\")\n",
    "    writer.write(val)\n",
    "\n",
    "    print(\"******** Saving Test set ********\")\n",
    "    fname = os.path.join(base_loc, \"test.tfrecord\")\n",
    "    writer = tf.data.experimental.TFRecordWriter(fname, \"ZLIB\")\n",
    "    writer.write(test)\n",
    "\n",
    "    print(\"******** Saving Training set ********\")\n",
    "    fname = os.path.join(base_loc, \"train.tfrecord\")\n",
    "    writer = tf.data.experimental.TFRecordWriter(fname, \"ZLIB\")\n",
    "    writer.write(train)\n",
    "\n",
    "    print(\"******** Finished saving dataset ********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "******** Saving Vocabulary ********\n******** Saving Validation set ********\n******** Saving Test set ********\n******** Saving Training set ********\n******** Finished saving dataset ********\n"
    }
   ],
   "source": [
    "save(vocab, train.map(serialize, -1), test.map(serialize, -1), val.map(serialize, -1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}